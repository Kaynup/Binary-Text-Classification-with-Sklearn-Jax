{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN Package Test Notebook\n",
    "\n",
    "This notebook tests all components of the RNN package:\n",
    "- Vanilla RNN\n",
    "- LSTM\n",
    "- GRU\n",
    "- Bidirectional wrapper\n",
    "- Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..', 'src')))\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "import numpy as np\n",
    "\n",
    "print(f\"JAX version: {jax.__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Import All Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from modelling.rnn import (\n",
    "    # Cells\n",
    "    VanillaRNNCell,\n",
    "    LSTMCell,\n",
    "    GRUCell,\n",
    "    # Layers\n",
    "    RNN,\n",
    "    LSTM,\n",
    "    GRU,\n",
    "    Bidirectional,\n",
    "    # Utilities\n",
    "    initialize_carry,\n",
    "    initialize_lstm_carry,\n",
    "    pad_sequences,\n",
    "    create_padding_mask,\n",
    ")\n",
    "\n",
    "print(\"✅ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Vanilla RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN cell\n",
    "cell = VanillaRNNCell(hidden_size=64)\n",
    "\n",
    "# Initialize\n",
    "batch_size = 2\n",
    "input_dim = 128\n",
    "h_0 = jnp.zeros((batch_size, 64))\n",
    "x_t = jax.random.normal(jax.random.PRNGKey(0), (batch_size, input_dim))\n",
    "\n",
    "# Get parameters\n",
    "params = cell.init(jax.random.PRNGKey(1), h_0, x_t)\n",
    "\n",
    "# Forward pass\n",
    "h_1, out = cell.apply(params, h_0, x_t)\n",
    "\n",
    "print(f\"Input shape: {x_t.shape}\")\n",
    "print(f\"Hidden shape: {h_1.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"✅ Vanilla RNN Cell works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: LSTM Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM cell\n",
    "lstm_cell = LSTMCell(hidden_size=64)\n",
    "\n",
    "# Initialize (LSTM needs both h and c)\n",
    "h_0 = jnp.zeros((batch_size, 64))\n",
    "c_0 = jnp.zeros((batch_size, 64))\n",
    "x_t = jax.random.normal(jax.random.PRNGKey(0), (batch_size, input_dim))\n",
    "\n",
    "# Get parameters\n",
    "params = lstm_cell.init(jax.random.PRNGKey(1), (h_0, c_0), x_t)\n",
    "\n",
    "# Forward pass\n",
    "(h_1, c_1), out = lstm_cell.apply(params, (h_0, c_0), x_t)\n",
    "\n",
    "print(f\"Input shape: {x_t.shape}\")\n",
    "print(f\"Hidden shape: {h_1.shape}\")\n",
    "print(f\"Cell shape: {c_1.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"✅ LSTM Cell works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: GRU Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRU cell\n",
    "gru_cell = GRUCell(hidden_size=64)\n",
    "\n",
    "# Initialize\n",
    "h_0 = jnp.zeros((batch_size, 64))\n",
    "x_t = jax.random.normal(jax.random.PRNGKey(0), (batch_size, input_dim))\n",
    "\n",
    "# Get parameters\n",
    "params = gru_cell.init(jax.random.PRNGKey(1), h_0, x_t)\n",
    "\n",
    "# Forward pass\n",
    "h_1, out = gru_cell.apply(params, h_0, x_t)\n",
    "\n",
    "print(f\"Input shape: {x_t.shape}\")\n",
    "print(f\"Hidden shape: {h_1.shape}\")\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"✅ GRU Cell works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 5: RNN Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RNN layer\n",
    "rnn = RNN(hidden_size=64, return_sequences=False)\n",
    "\n",
    "# Sample input: (batch, seq_len, input_dim)\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 10, 128))\n",
    "\n",
    "# Initialize\n",
    "params = rnn.init(jax.random.PRNGKey(1), x)\n",
    "\n",
    "# Forward pass\n",
    "output = rnn.apply(params, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape (final state): {output.shape}\")\n",
    "\n",
    "# Test with return_sequences=True\n",
    "rnn_seq = RNN(hidden_size=64, return_sequences=True)\n",
    "params_seq = rnn_seq.init(jax.random.PRNGKey(1), x)\n",
    "output_seq = rnn_seq.apply(params_seq, x)\n",
    "\n",
    "print(f\"Output shape (all states): {output_seq.shape}\")\n",
    "print(f\"✅ RNN Layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 6: LSTM Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM layer\n",
    "lstm = LSTM(hidden_size=64, return_sequences=False)\n",
    "\n",
    "# Sample input\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 10, 128))\n",
    "\n",
    "# Initialize and forward\n",
    "params = lstm.init(jax.random.PRNGKey(1), x)\n",
    "output = lstm.apply(params, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"✅ LSTM Layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 7: GRU Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GRU layer\n",
    "gru = GRU(hidden_size=64, return_sequences=False)\n",
    "\n",
    "# Sample input\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 10, 128))\n",
    "\n",
    "# Initialize and forward\n",
    "params = gru.init(jax.random.PRNGKey(1), x)\n",
    "output = gru.apply(params, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"✅ GRU Layer works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 8: Bidirectional Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create bidirectional LSTM\n",
    "base_lstm = LSTM(hidden_size=64, return_sequences=True)\n",
    "bi_lstm = Bidirectional(base_lstm, merge_mode='concat')\n",
    "\n",
    "# Sample input\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 10, 128))\n",
    "\n",
    "# Initialize and forward\n",
    "params = bi_lstm.init(jax.random.PRNGKey(1), x)\n",
    "output = bi_lstm.apply(params, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape (concat): {output.shape}\")  # Should be (2, 10, 128) - 64*2\n",
    "print(f\"✅ Bidirectional wrapper works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 9: Utilities - Padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable-length sequences\n",
    "seq1 = jnp.array([1, 2, 3])\n",
    "seq2 = jnp.array([4, 5])\n",
    "seq3 = jnp.array([6, 7, 8, 9])\n",
    "\n",
    "# Pad sequences\n",
    "padded = pad_sequences([seq1, seq2, seq3], padding='post', value=0)\n",
    "\n",
    "print(\"Padded sequences:\")\n",
    "print(padded)\n",
    "print(f\"Shape: {padded.shape}\")\n",
    "print(f\"✅ Padding works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 10: Utilities - Masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mask for padded sequences\n",
    "lengths = jnp.array([3, 2, 4])\n",
    "mask = create_padding_mask(lengths, max_len=4)\n",
    "\n",
    "print(\"Padding mask:\")\n",
    "print(mask)\n",
    "print(f\"Shape: {mask.shape}\")\n",
    "print(f\"✅ Masking works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 11: Complete Sentiment Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a complete sentiment classifier using LSTM\n",
    "class SentimentLSTM(nn.Module):\n",
    "    vocab_size: int = 8000\n",
    "    embed_dim: int = 128\n",
    "    hidden_size: int = 64\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # x: (batch, seq_len) - token IDs\n",
    "        \n",
    "        # 1. Embed tokens\n",
    "        x = nn.Embed(self.vocab_size, self.embed_dim)(x)\n",
    "        # x: (batch, seq_len, embed_dim)\n",
    "        \n",
    "        # 2. Process with LSTM\n",
    "        x = LSTM(self.hidden_size, return_sequences=False)(x)\n",
    "        # x: (batch, hidden_size)\n",
    "        \n",
    "        # 3. Classification head\n",
    "        x = nn.Dense(1)(x)\n",
    "        # x: (batch, 1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = SentimentLSTM()\n",
    "\n",
    "# Sample input (token IDs)\n",
    "x = jnp.ones((2, 50), dtype=jnp.int32)\n",
    "\n",
    "# Initialize\n",
    "params = model.init(jax.random.PRNGKey(0), x)\n",
    "\n",
    "# Forward pass\n",
    "logits = model.apply(params, x)\n",
    "predictions = nn.sigmoid(logits)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Logits shape: {logits.shape}\")\n",
    "print(f\"Predictions: {predictions}\")\n",
    "print(f\"✅ Complete sentiment classifier works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 12: Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a stacked LSTM model\n",
    "class StackedLSTM(nn.Module):\n",
    "    hidden_sizes: list = (64, 32)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        # First LSTM: return all sequences\n",
    "        x = LSTM(self.hidden_sizes[0], return_sequences=True)(x)\n",
    "        \n",
    "        # Second LSTM: return final state only\n",
    "        x = LSTM(self.hidden_sizes[1], return_sequences=False)(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model\n",
    "model = StackedLSTM()\n",
    "\n",
    "# Sample input\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 50, 128))\n",
    "\n",
    "# Initialize and forward\n",
    "params = model.init(jax.random.PRNGKey(1), x)\n",
    "output = model.apply(params, x)\n",
    "\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")  # Should be (2, 32)\n",
    "print(f\"✅ Stacked LSTM works!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 13: Compare Parameter Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_params(params):\n",
    "    \"\"\"Count total parameters in a model\"\"\"\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "\n",
    "# Create models with same hidden size\n",
    "hidden_size = 64\n",
    "input_dim = 128\n",
    "x = jax.random.normal(jax.random.PRNGKey(0), (2, 10, input_dim))\n",
    "\n",
    "# RNN\n",
    "rnn = RNN(hidden_size=hidden_size)\n",
    "rnn_params = rnn.init(jax.random.PRNGKey(0), x)\n",
    "rnn_count = count_params(rnn_params)\n",
    "\n",
    "# LSTM\n",
    "lstm = LSTM(hidden_size=hidden_size)\n",
    "lstm_params = lstm.init(jax.random.PRNGKey(0), x)\n",
    "lstm_count = count_params(lstm_params)\n",
    "\n",
    "# GRU\n",
    "gru = GRU(hidden_size=hidden_size)\n",
    "gru_params = gru.init(jax.random.PRNGKey(0), x)\n",
    "gru_count = count_params(gru_params)\n",
    "\n",
    "print(\"Parameter Counts:\")\n",
    "print(f\"RNN:  {rnn_count:,} parameters\")\n",
    "print(f\"LSTM: {lstm_count:,} parameters ({lstm_count/rnn_count:.1f}x RNN)\")\n",
    "print(f\"GRU:  {gru_count:,} parameters ({gru_count/rnn_count:.1f}x RNN)\")\n",
    "print(f\"\\n✅ Parameter comparison complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "All tests passed! ✅\n",
    "\n",
    "You now have a complete RNN package with:\n",
    "- ✅ Vanilla RNN (simple, fast)\n",
    "- ✅ LSTM (handles long-term dependencies)\n",
    "- ✅ GRU (balanced between RNN and LSTM)\n",
    "- ✅ Bidirectional wrapper\n",
    "- ✅ Utilities for padding and masking\n",
    "\n",
    "Next steps:\n",
    "1. Use these components in your sentiment classification task\n",
    "2. Compare performance with your BoW baseline\n",
    "3. Experiment with different architectures (stacked, bidirectional, etc.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
